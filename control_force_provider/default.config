algorithm: pfm
visualize: true
rcm_topic: /pivot_controller/desired/pivot_point
pfm:
  attraction_strength: 10e-4
  attraction_distance: 0.1
  repulsion_strength: 10e-4
  repulsion_distance: 0.15
  z_translation_strength: 10e-5
  min_rcm_distance: 0.05
obstacles:
  ob1:
    type: waypoints
    rcm: [ 0.5,0.05,0.43 ]
    waypoints: [ 0.3,-0.2,0.4 ,0.4,0.2,0.1, 0.2,0,0.2, 0.1,0.1,0.3 ]
    speed: 0.15
  ob2:
    type: waypoints
    rcm: [ 0.5,-0.05,0.43 ]
    waypoints: [ 0.2,-0.3,0.3 ,0.3,0.1,0.2, 0.3,0,0.1, 0.2,0.1,0.3 ]
    speed: 0.15
rl:
  type: dqn
  train: true
  discount_factor: 0.1
  batch_size: 128
  updates_per_step: 10
  # state_pattern syntax:
  #   x(arg1arg2...)y(arg1arg2...)... -> x,y are the vector ids. Args can be omitted for default values.
  #   possible ids:
  #     ree           = robot position
  #     rve           = robot velocity
  #     rpp           = robot pivot point (rcm)
  #     oee           = obstacle position
  #     ove           = obstacle velocity
  #     opp           = obstacle pivot point (rcm)
  #     gol           = goal
  #     tim           = elapsed time in seconds
  #   possible args:
  #     hx            = track a history of the x last values (default = 1)
  #     sx            = stride/distance between two values in the history (default = 0)
  state_pattern: ree()rpp()oee(h10)opp()gol()tim()
  max_force: 10e-4
  interval_duration: 30 #ms
  exploration_angle_sigma: 45
  exploration_magnitude_sigma: 0.0005
  exploration_decay: 0.9999
  output_directory: ""
  reward_function:
    dc: 0.02
    mc: 2
    max_penalty: 10000
    dg: 0.005
    rg: 10
  state_augmenter:
    ob_sigma: 0.000001
    ob_max_noise: 0.002
  dqn:
    layer_size: 64
    replay_buffer_size: 10000
    target_network_update_rate: 10