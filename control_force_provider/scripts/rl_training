#! /usr/bin/env python

import rospy
import yaml
import signal
import torch
import control_force_provider.rl as rl
from control_force_provider.native import RLEnvironment
from argparse import ArgumentParser
from functools import reduce
from control_force_provider_msgs.srv import UpdateNetwork, UpdateNetworkResponse

rl_context = None
stop = False
action_dim = 3


def service_handle(request):
    state_dict = {}
    for field in request.__slots__:
        value = getattr(request, field)
        state_dict[field] = torch.tensor(value).unsqueeze(0)
    action = rl_context.update(state_dict).tolist()
    action.append(0)
    return UpdateNetworkResponse(action)


def signal_handle():
    global stop
    stop = True
    if rl_context is not None:
        rl_context.save()


def prepare_config(config):
    num_obstacles = 0
    non_obstacles_keys = ["data"]
    for key in config["obstacles"]:
        if key not in non_obstacles_keys:
            num_obstacles += 1
    for key in ["max_force", "workspace_bb"]:
        config["rl"][key] = config[key]
    config = config["rl"]
    state_augmenter = rl.StateAugmenter(rl.create_state_mapping(config["state_pattern"], num_obstacles), **config["state_augmenter"])
    config["state_dim"] = reduce(lambda x, y: x + y, (x[1] for x in state_augmenter.mapping.values()))
    config["action_dim"] = action_dim
    config["reward_function"] = rl.RewardFunction(fmax=config["max_force"], interval_duration=config["interval_duration"], dg=config["goal_reached_threshold_distance"], **config["reward_function"])
    config["state_augmenter"] = state_augmenter
    flattened_config = {}
    for k1, v1 in config.items():
        if isinstance(v1, dict):
            for k2, v2 in v1.items():
                flattened_config[k2] = v2
        else:
            flattened_config[k1] = v1
    return flattened_config


def main():
    global rl_context, stop
    arg_parser = ArgumentParser()
    arg_parser.add_argument("-s", "--service", action="store_true")
    arg_parser.add_argument("-tw", "--transfer_weights", type=str)
    arg_parser.add_argument("-v", "--visualize", action="store_true")
    args, _ = arg_parser.parse_known_args()
    config_file = rospy.get_param("control_force_provider/config")
    with open(config_file, "r") as f:
        config = yaml.safe_load(f)
    if config["algorithm"] != "rl":
        return
    if args.service:
        config["robot_batch"] = 1
    config = prepare_config(config)
    rl_context = rl.context_mapping[config["type"]](**config)
    rl_context.load()
    rospy.init_node("rl_training", anonymous=True)
    if args.transfer_weights is not None:
        new_rl_context = rl.context_mapping[config["type"]](**config)
        for attribute_key, value in vars(rl_context).items():
            if isinstance(value, torch.nn.Module):
                setattr(new_rl_context, attribute_key, value)
            elif isinstance(value, list):
                for i, list_item in enumerate(value):
                    if isinstance(list_item, torch.nn.Module):
                        getattr(new_rl_context, attribute_key)[i] = list_item
        new_rl_context.output_dir = args.transfer_weights
        new_rl_context.save()
        return
    if args.service:
        rospy.Service("update_network", UpdateNetwork, service_handle)
        rospy.spin()
    else:
        rl_environment = RLEnvironment(config_file, [0.3, 0., 0.45], False, args.visualize)
        state_dict = rl_environment.observe(torch.zeros((rl_context.robot_batch, 3), device=rl.DEVICE))
        while not rospy.is_shutdown():
            action = rl_context.update(state_dict)
            state_dict = rl_environment.observe(action)
            if args.visualize and isinstance(rl_context, rl.HierarchicalRLContext):
                rl_environment.set_custom_marker("goals", rl_context.goals[0])
    rl_context.save()


if __name__ == '__main__': main()
